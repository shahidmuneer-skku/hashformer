from enum import Enum
import functools
from functools import wraps

import torch
import torch.nn.functional as F
from torch import nn, einsum
from einops import rearrange, repeat, pack, unpack
from einops.layers.torch import Rearrange
import timm
from timm.models.layers import to_2tuple, trunc_normal_
from timm.models.vision_transformer import Mlp, PatchEmbed, Block
import numpy as np
from beartype import beartype
from torch.nn import Linear
from .positional_embeddings import SinCosPositionalEmbedding
# from beartype.typing import Tuple, Optional, Union, Sequence
# from timm.layers import get_3d_sincos_pos_embed
from torchaudio.transforms import Spectrogram
from torchvision.models import resnet50
import math
from typing import Union, Optional, Callable, Tuple, List, Sequence

import torch
from einops.layers.torch import Rearrange
from torch import Tensor, nn, Size
from torch.nn import Conv3d, ModuleList
from torch.nn import functional as F

Shape = Union[Size, List[int], Tuple[int, ...]]
ModuleFactory = Union[Callable[[], nn.Module], Callable[[int], nn.Module]]


# option A – xFormers (pip install xformers)
# from xformers.ops.fmha import rotary  # rotary.apply_rotary
# cos, sin = rotary.get_fixed_cos_sin(seq_len, head_dim, device)

# option B – flash-attn v2 (pip install flash-attn)
# from flash_attn.ops.rotary import apply_rotary_pos_emb as apply_rotary
# from flash_attn.ops.rotary import rotary_embedding  # builds cos/sin

# option C – write 6 lines yourself (shown below)

# constants

class TokenTypes(Enum):
    AUDIO_FUSION = 0
    AUDIO = 1
    VIDEO_FUSION = 2
    VIDEO = 3
    TEXT_FUSION = 4
    TEXT = 5
    IMAGE_FUSION = 6
    IMAGE = 7
    GLOBAL = 8

# functions

def exists(val):
    return val is not None

def default(*args):
    for arg in args:
        if exists(arg):
            return arg
    return None

def round_down_nearest_multiple(n, divisor):
    return n // divisor * divisor

def pair(t):
    return (t, t) if not isinstance(t, tuple) else t

def cum_mul(it):
    return functools.reduce(lambda x, y: x * y, it, 1)

def divisible_by(numer, denom):
    return (numer % denom) == 0

# decorators

def once(fn):
    called = False
    @wraps(fn)
    def inner(x):
        nonlocal called
        if called:
            return
        called = True
        return fn(x)
    return inner

print_once = once(print)

# bias-less layernorm


def no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
            stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


class Linear(nn.Module):

    def __init__(self, in_features: int, out_features: int, bias: bool = True,
        build_activation: Optional[ModuleFactory] = None,
        build_normalization: Optional[ModuleFactory] = None,
        normalization_after_activation: bool = False,
        dropout_rate: float = 0.
    ):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features, bias)

        self.has_act = build_activation is not None
        if self.has_act:
            self.activation = build_activation()
        else:
            self.activation = None

        self.has_norm = build_normalization is not None
        if self.has_norm:
            self.normalization = build_normalization()
            self.norm_after_act = normalization_after_activation
        else:
            self.normalization = None

        self.has_dropout = dropout_rate > 0
        if self.has_dropout:
            self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x: Tensor) -> Tensor:
        x = self.linear(x)
        if self.has_act and self.has_norm:
            if self.norm_after_act:
                x = self.activation(x)
                x = self.normalization(x)
            else:
                x = self.normalization(x)
                x = self.activation(x)
        elif self.has_act and not self.has_norm:
            x = self.activation(x)
        elif not self.has_act and self.has_norm:
            x = self.normalization(x)

        if self.has_dropout:
            x = self.dropout(x)
        return x




class MLP(nn.Module):

    def __init__(self, neurons: Sequence[int],
        build_activation: Optional[ModuleFactory] = None, dropout_rate: float = 0.
    ):
        super().__init__()
        n_features = neurons[1:]
        self.layers: ModuleList[Linear] = ModuleList(
            [Linear(neurons[i], neurons[i + 1], True, build_activation, None,
                False, dropout_rate
            ) for i in range(len(n_features) - 1)
            ] + [
                Linear(neurons[-2], neurons[-1], True)
            ]
        )

    def forward(self, x: Tensor) -> Tensor:
        for layer in self.layers:
            x = layer(x)
        return x

class Block3d(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
        init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,
        attn_head_dim=None
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim=dim, heads=num_heads)
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = MLP(
            neurons=[dim, mlp_hidden_dim, dim],
            build_activation=act_layer,
            dropout_rate=drop
        )

        if init_values > 0:
            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)
            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)
        else:
            self.gamma_1, self.gamma_2 = None, None

    def forward(self, x):
        if self.gamma_1 is None:
            x = x + self.attn(self.norm1(x))
            x = x + self.mlp(self.norm2(x))
        else:
            x = x + (self.gamma_1 * self.attn(self.norm1(x)))
            x = x + (self.gamma_2 * self.mlp(self.norm2(x)))
        return x


class VisualDecoder16x16(nn.Module):
    def __init__(self, img_size=224, patch_size=16, n_frames=25, embed_dim=384, depth=8,
        num_heads=6, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,
        norm_layer="LayerNorm", init_values=1., tubelet_size=5, encoder_embed_dim=768
    ):
        super().__init__()
        output_dim = 3 * tubelet_size * patch_size * patch_size
        self.patch_size = patch_size
        self.tubelet_size = tubelet_size
        self.n_patch_h = img_size // patch_size
        self.n_patch_w = img_size // patch_size
        self.embed_dim = embed_dim
        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        if norm_layer == "LayerNorm":
            self.norm_layer = nn.LayerNorm
            self.norm = self.norm_layer(embed_dim)
        else:
            raise NotImplementedError("Only LayerNorm is supported")
        # max_pos_video=784
        max_pos_video=784
        # self.video_grid = (
        #     max_pos_video // tubelet_size,          # temporal grid
        #     224 // self.n_patch_h,                  # height grid — or video.shape[-2] if dynamic
        #     224 // self.n_patch_w                    # width grid — or video.shape[-1] if dynamic
        # )
        # sine-cosine positional embeddings
        # self.pos_embedding = nn.Parameter(torch.zeros(1, 1, embed_dim))
        # pos_3d = get_3d_sincos_pos_embed(
        #     embed_dim = (self.n_patch_h * self.n_patch_w * (n_frames // tubelet_size)),
        #     grid_size = self.video_grid,
        #     cls_token = False
        #   )   # returns [T*H*W, dim] numpy\   
          
        self.pos_embedding = SinCosPositionalEmbedding(
            (max_pos_video, embed_dim), dropout_rate=0.) 
        
        # self.pos_embedding = SinCosPositionalEmbedding(
        #     (self.n_patch_h * self.n_patch_w * (n_frames // tubelet_size), embed_dim), dropout_rate=0.) 
      
        
        self.blocks = nn.ModuleList([
            Block3d(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, norm_layer=self.norm_layer,
                init_values=init_values
            ) for _ in range(depth)])

        self.head = nn.Linear(embed_dim, output_dim)
        self.apply(self._init_weights)
        no_grad_trunc_normal_(self.mask_token, mean=0., std=0.02, a=-0.02, b=0.02)
        
        self.decoder_patch_embed = nn.Linear(encoder_embed_dim, embed_dim)

    @staticmethod
    def _init_weights(m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def unpatch_to_img(self, x: Tensor) -> Tensor:
        # x: (Batch, No. batches, Prod of cube size * C)
        x = rearrange(x, "b n (c p) -> b n p c", c=3)
        # x: (Batch, No. batches, Prod of cube size, C)
        x = rearrange(x, "b (t h w) (p0 p1 p2) c -> b c (t p0) (h p1) (w p2)", p0=self.tubelet_size,
            p1=self.patch_size, p2=self.patch_size, h=self.n_patch_h, w=self.n_patch_w)
        # x: (B, C, T, H, W)
        return x

    def forward_features(self, x):
        for block in self.blocks:
            x = block(x)

        # if return_token_num > 0:
        #     x = x[:, -return_token_num:]

        x = self.norm(x)
        x = self.head(x)
        # x: (B, N_mask, C)
        return x

    def forward(self, x):
        # mask: 0 -> masked, 1 -> visible
        # b, n, c = x.shape
        # expand_pos_embed = self.pos_embedding.emb.data.expand(b, -1, -1)
        # pos_emb_vis = expand_pos_embed[mask].view(b, -1, c)
        # pos_emb_mask = expand_pos_embed[~mask].view(b, -1, c)
        # x = torch.cat([x + pos_emb_vis, self.mask_token + pos_emb_mask], dim=1)

        # mask_num = pos_emb_mask.shape[1]

        # print(x.shape)
        x = self.decoder_patch_embed(x)
        # print(x.shape)
        # print(x.shape)
        x = self.pos_embedding(x)
        # print(x.shape)
        x = self.forward_features(x)
        return x



class VisualDecoder(nn.Module):
    """
    Re-assemble ViT/Tubelet tokens into a (B, 3, T, H, W) video.
    Works for any n_frames ≥ tubelet_size.
    """
    def __init__(
        self,
        img_size:     int = 224,
        patch_size:   int = 16,
        n_frames:     int = 25,     # works for 16, 25, 32, …
        tubelet_size: int = 2,
        embed_dim:    int = 384,
        depth:        int = 8,
        num_heads:    int = 6,
        mlp_ratio:    float = 4.,
        qkv_bias:     bool = False,
        qk_scale=None,
        drop_rate:    float = 0.,
        attn_drop_rate: float = 0.,
        norm_layer:   str = "LayerNorm",
        init_values:  float = 1.,
        encoder_embed_dim: int = 768,
    ):
        super().__init__()

        # ------------------------------------------------------------------ #
        # Geometry
        # ------------------------------------------------------------------ #
        self.patch_size   = patch_size
        self.tubelet_size = tubelet_size
        self.n_patch_h    = img_size // patch_size       # e.g. 224//16 = 14
        self.n_patch_w    = img_size // patch_size       # same

        # token count produced by the encoder
        self.tokens_per_frame = self.n_patch_h * self.n_patch_w        # 14×14 = 196
        self.time_grid        = math.ceil(n_frames / tubelet_size)     # ceil so odd clips work
        self.max_pos_video    = 1176 #self.tokens_per_frame * self.time_grid

        # ------------------------------------------------------------------ #
        # Layers
        # ------------------------------------------------------------------ #
        output_dim = 3 * tubelet_size * patch_size * patch_size

        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))

        if norm_layer == "LayerNorm":
            self.norm_layer = nn.LayerNorm
            self.norm       = self.norm_layer(embed_dim)
        else:
            raise NotImplementedError("Only LayerNorm is supported")

        self.pos_embedding = SinCosPositionalEmbedding(
            (self.max_pos_video, embed_dim), dropout_rate=0.0
        )

        self.blocks = nn.ModuleList([
            Block3d(
                dim          = embed_dim,
                num_heads    = num_heads,
                mlp_ratio    = mlp_ratio,
                qkv_bias     = qkv_bias,
                qk_scale     = qk_scale,
                drop         = drop_rate,
                attn_drop    = attn_drop_rate,
                norm_layer   = self.norm_layer,
                init_values  = init_values,
            )
            for _ in range(depth)
        ])

        self.head = nn.Linear(embed_dim, output_dim)
        self.decoder_patch_embed = nn.Linear(encoder_embed_dim, embed_dim)

        self.apply(self._init_weights)
        no_grad_trunc_normal_(self.mask_token, mean=0., std=0.02, a=-0.02, b=0.02)

    # ----------------------------------------------------------------------#
    # Helpers
    # ----------------------------------------------------------------------#
    @staticmethod
    def _init_weights(m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.weight, 1.0)
            nn.init.constant_(m.bias,   0)

    # ----------------------------------------------------------------------#
    # Un-patch tokens back to video
    # ----------------------------------------------------------------------#
    def unpatch_to_img(self, x: Tensor) -> Tensor:
        """
        x : (B, N, 3·patch_volume) where
            patch_volume = tubelet_size · patch_size²
        returns (B, 3, T, H, W)
        """
        x = rearrange(x, "b n (c p) -> b n p c", c=3)  # (B,N,patch_vol,3)

        B, N, P, C = x.shape
        spatial_tokens = self.tokens_per_frame          # 196 for ViT/16
        assert N % spatial_tokens == 0, (
            f"Unexpected token count {N}. "
            f"Expected a multiple of spatial grid {spatial_tokens}."
        )
        t = N // spatial_tokens                         # derive time grid

        x = rearrange(
            x,
            "b (t h w) (p0 p1 p2) c -> b c (t p0) (h p1) (w p2)",
            t  = t,
            p0 = self.tubelet_size,
            p1 = self.patch_size,
            p2 = self.patch_size,
            h  = self.n_patch_h,
            w  = self.n_patch_w,
        )
        return x                                        # (B,3,T,H,W)

    # ----------------------------------------------------------------------#
    # Forward pass
    # ----------------------------------------------------------------------#
    def forward_features(self, x: Tensor) -> Tensor:
        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)
        x = self.head(x)
        return x

    def forward(self, x: Tensor) -> Tensor:
        """
        x : (B, N_enc, encoder_embed_dim) — tokens from the encoder
        returns decoded tokens (before un-patching)
        """
        x = self.decoder_patch_embed(x)     # project to decoder dim
        x = self.pos_embedding(x)           # add 3-D sinusoidal PE
        x = self.forward_features(x)        # transformer
        return x


class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()

        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x

class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = self.attn = Attention(dim=dim, heads=num_heads)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x):
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

class AudioDecoder(nn.Module):
    def __init__(self, num_patches=(1024 * 128 // 256), patch_size=16, encoder_embed_dim=512*2, 
                 decoder_embed_dim=512, num_heads=16, decoder_depth=8, mlp_ratio=4.,
                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,
                 n_mfcc: int    = 40,     # mel bins (H)
                 t_frames: int  = 24,    # total time frames (W)
                 patch_time: int = 32,   # frames per token (W / gw)
                 embed_dim: int = 512):
        super().__init__()
        timm.models.vision_transformer.PatchEmbed = PatchEmbed
        timm.models.vision_transformer.Block = Block
        self.n_mfcc     = n_mfcc
        self.t_frames   = t_frames
        self.patch_time = patch_time
        self.embed_dim  = embed_dim
        self.num_patches = num_patches
        
        decoder_embed_dim= n_mfcc  * patch_time
        self.decoder_embed = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=True)
        
        self.decoder_modality = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))
        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches, decoder_embed_dim), requires_grad=True)
        
        self.blocks = nn.Sequential(*[
            Block(
                dim=decoder_embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=0.,
                norm_layer=nn.LayerNorm,
                act_layer=nn.GELU)
            for _ in range(decoder_depth)])
        
        self.decoder_norm = nn.LayerNorm(decoder_embed_dim)
        self.decoder_pred = nn.Linear(decoder_embed_dim, decoder_embed_dim, bias=True)
        
        self.initialize_weights()
    def unpatch_to_mfcc(self, audio, audio_input_shape):
       
        # 2) reshape into 2-D patches: [B, gw, n_mfcc, patch_time]
        # B,N,T = audio_input.shape
        audio = audio.mean(dim=1)
        B, N, _ = audio_input_shape
        mfcc = audio.view(B,self.n_mfcc,
                               self.patch_time)
        return mfcc

    def forward(self, audio):
        # audio = torch.flatten(audio, start_dim=1,end_dim=2)
        # print(audio.shape)

        x = self.decoder_embed(audio)
        for blk in self.blocks:
            x = blk(x)
        x = self.decoder_norm(x)
        x = self.decoder_pred(x)
        
        return x
        
    def initialize_weights(self):
        # decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], 8, int(self.num_patches/8))
        # self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))
        
        torch.nn.init.normal_(self.decoder_modality, std=.02)
        
        self.apply(self._init_weights)
        
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            # we use xavier_uniform following official JAX ViT:
            torch.nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)


class LayerNorm(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(dim))
        self.register_buffer("beta", torch.zeros(dim))

    def forward(self, x):
        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)

# geglu feedforward

class GEGLU(nn.Module):
    def forward(self, x):
        x, gate = x.chunk(2, dim = -1)
        return F.gelu(gate) * x

class SELU(nn.Module):
    def forward(self, x):
        x, gate = x.chunk(2, dim = -1)
        return F.selu(gate) * x
def FeedForward(dim, mult = 4):
    inner_dim = int(dim * mult * 2 / 3)
    return nn.Sequential(
        LayerNorm(dim),
        nn.Linear(dim, inner_dim * 2, bias = False),
        GEGLU(),
        LayerNorm(inner_dim),
        nn.Linear(inner_dim, dim, bias = False)
    )

def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float32)
    omega /= embed_dim / 2.
    omega = 1. / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product

    emb_sin = np.sin(out) # (M, D/2)
    emb_cos = np.cos(out) # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb


# implemented from https://gist.github.com/srmsoumya/ce14ccdf50a3089a6ebe860789ae7dd3
# attention
def get_3d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: 3d tuple of grid size: t, h, w
    return:
    pos_embed: L, D
    """

    assert embed_dim % 16 == 0

    t_size, h_size, w_size = grid_size

    w_embed_dim = embed_dim // 16 * 6
    h_embed_dim = embed_dim // 16 * 6
    t_embed_dim = embed_dim // 16 * 4

    w_pos_embed = get_1d_sincos_pos_embed_from_grid(w_embed_dim, np.arange(w_size))
    h_pos_embed = get_1d_sincos_pos_embed_from_grid(h_embed_dim, np.arange(h_size))
    t_pos_embed = get_1d_sincos_pos_embed_from_grid(t_embed_dim, np.arange(t_size))

    w_pos_embed = np.tile(w_pos_embed, (t_size * h_size, 1))
    h_pos_embed = np.tile(np.repeat(h_pos_embed, w_size, axis=0), (t_size, 1))
    t_pos_embed = np.repeat(t_pos_embed, h_size * w_size, axis=0)

    pos_embed = np.concatenate((w_pos_embed, h_pos_embed, t_pos_embed), axis=1)

    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed

def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    
    assert embed_dim % 2 == 0

    # use half of dimensions to encode grid_h
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)

    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)
    return emb


def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: int of the grid height and width
    return:
    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)
    """
    grid_h = np.arange(grid_size, dtype=np.float32)
    grid_w = np.arange(grid_size, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)  # here w goes first
    grid = np.stack(grid, axis=0)

    grid = grid.reshape([2, 1, grid_size, grid_size])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


def build_rope_cache(seq_len: int, dim: int, device):
    """Returns cos[seq_len, dim], sin[seq_len, dim] tensors."""
    theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device) / dim))
    pos   = torch.arange(seq_len, device=device).float()
    freqs = torch.outer(pos, theta)                         # [seq, dim/2]
    emb   = torch.cat((freqs, freqs), dim=-1)               # repeat for even/odd
    cos, sin = emb.cos(), emb.sin()                         # [seq, dim]
    return cos, sin

class RopeCache(nn.Module):
    """
    Keeps cos/sin lookup tables and auto‑expands them when needed.
    Call   cos, sin = cache(seq_len, device, dtype)
    """
    def __init__(self, dim: int, base: int = 10_000):
        super().__init__()
        self.dim  = dim
        self.base = base
        # start with a minimal table so first forward builds exactly what is needed
        self.register_buffer("_cos", torch.empty(0), persistent=False)
        self.register_buffer("_sin", torch.empty(0), persistent=False)

    def forward(self, seq_len: int, device, dtype):
        # if the cache is too small (or on a wrong device / dtype) ‑→ rebuild
        if (seq_len > self._cos.shape[0]         or
            self._cos.device != device           or
            self._cos.dtype  != dtype):
            self._build(seq_len, device, dtype)
        return self._cos[:seq_len], self._sin[:seq_len]

    @torch.no_grad()
    def _build(self, seq_len, device, dtype):
        theta = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=device, dtype=dtype) / self.dim))
        pos   = torch.arange(seq_len, device=device, dtype=dtype)
        freqs = torch.outer(pos, theta)                # [seq, dim/2]
        emb   = torch.cat((freqs, freqs), dim=-1)      # [seq, dim]
        cos, sin = emb.cos(), emb.sin()
        self._cos = cos
        self._sin = sin

def apply_rotary(x, cos, sin):
    """
    x: [B, H, N, D]   (after you reshape with einops)
    cos/sin: [N, D]
    """
    cos = cos[None, None, :, :]         # broadcast over batch & heads
    sin = sin[None, None, :, :]
    x1, x2 = x[..., ::2], x[..., 1::2]  # even, odd
    x_rot  = torch.stack((-x2, x1), dim=-1).reshape_as(x)
    return x * cos + x_rot * sin


def get_positional_embeddings(seq_len, dim):
    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
    pos = torch.arange(seq_len, dtype=torch.float).unsqueeze(1)
    sinusoid_inp = torch.einsum("i,j->ij", pos, inv_freq)
    embeddings = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)
    return embeddings

def apply_RoPE(x, positional_embeddings):
    seq_len, dim = x.shape[1], x.shape[2]
    x_rotated = torch.einsum("bnd,nd->bnd", x, positional_embeddings)
    return x_rotated
    
class Attention(nn.Module):
    def __init__(
        self,
        dim,
        dim_head = 64,
        heads = 8
    ):
        super().__init__()
        self.scale = dim_head ** -0.5
        self.heads = heads
        inner_dim = dim_head * heads
        dropout=0.1
        self.norm = LayerNorm(dim)

        # self.to_q = nn.Linear(dim, inner_dim, bias = False)
        # self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)
        

        self.to_q = nn.Sequential(
            nn.Linear(dim, inner_dim, bias = False),
            nn.Dropout(dropout)
        )
        self.to_kv = nn.Sequential(
            nn.Linear(dim, inner_dim * 2, bias = False),
            nn.Dropout(dropout)
        )
        
        self.attn_drop = nn.Dropout(0.1)
        self.to_out = nn.Linear(inner_dim, dim, bias = False)
          
        self.proj_drop = nn.Dropout(dropout)
           # Add bilinear transformation matrix
        # self.W_bilinear = nn.Parameter(torch.randn(self.heads, dim_head, dim_head))
        # Initialize as identity for stability
        # nn.init.eye_(self.W_bilinear.view(-1, dim_head))
        
        
        # self.attn_dropout = nn.Dropout(dropout)  # Dropout on attention scores if needed
        # NEW — one cache per Attention layer
        self.rope = RopeCache(dim_head, base=10_000)

    @staticmethod
    def _apply_rotary(x, cos, sin):
        """
        x : [B, H, N, D]      cos/sin : [N, D]
        """
        cos, sin = map(lambda t: t[None, None, :, :], (cos, sin))   # broadcast
        x1, x2   = x[..., ::2], x[..., 1::2]
        x_rot    = torch.stack((-x2, x1), dim=-1).reshape_as(x)
        return (x * cos) + (x_rot * sin)


    def forward(
        self,
        x,
        context = None,
        attn_mask = None
    ):
        x = self.norm(x)
        kv_x = default(context, x)

        q, k, v = (self.to_q(x), *self.to_kv(kv_x).chunk(2, dim = -1))
         
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))

        # # this method is opted from https://medium.com/%40DataDry/decoding-rotary-positional-embeddings-rope-the-secret-sauce-for-smarter-transformers-193cbc01e4ed
        seq_len = q.shape[2]
        device = x.device
       
        # This method is from chatgpt
        # ─── Rotary Positional Embeddings ──────────────────────
        # Rotary for query / key of *different* lengths
        seq_q, seq_k = q.shape[-2], k.shape[-2]
        max_len      = max(seq_q, seq_k)
        cos_base, sin_base = self.rope(max_len, device=x.device, dtype=x.dtype)

        q = self._apply_rotary(q, cos_base[:seq_q], sin_base[:seq_q])
        k = self._apply_rotary(k, cos_base[:seq_k], sin_base[:seq_k])
        # ─── Scaled dot‑product attention ─────────────────────
        
        
        q = q * self.scale

        # k_transformed = torch.einsum('bhnd,hde->bhne', k, self.W_bilinear)
        # sim = einsum('b h i d, b h j d -> b h i j', q, k_transformed)

        sim = einsum('b h i d, b h j d -> b h i j', q, k)

        if exists(attn_mask):
            sim = sim.masked_fill(~attn_mask, -torch.finfo(sim.dtype).max)

        attn = sim.softmax(dim = -1)
        attn = self.attn_drop(attn)

        out = einsum('b h i j, b h j d -> b h i d', attn, v)

        out = rearrange(out, 'b h n d -> b n (h d)')
        out = self.to_out(out)
        out = self.proj_drop(out)

        return out

# main class

class MaskEncoder(nn.Module):
    def __init__(
        self,
        dim,
        depth,
        dim_head = 64,
        heads = 8,
        ff_mult = 4,
        num_fusion_tokens = 16,
        audio_patch_size: Union[int, Tuple[int, int]] = 16,
        video_patch_size: Union[int, Tuple[int, int]] = 16,
        video_temporal_patch_size = 4,
        video_channels = 3,
        spec_n_fft = 128,
        spec_power = 2,
        spec_win_length = 24,
        spec_hop_length = None,
        spec_pad = 0,
        spec_center = True,
        spec_pad_mode = 'reflect',
        spec_aug_stretch_factor = 0.8,
        spec_aug_freq_mask = 80,
        spec_aug_time_mask = 80,
        return_token_types: Tuple[TokenTypes] = (TokenTypes.AUDIO_FUSION, TokenTypes.AUDIO, TokenTypes.VIDEO_FUSION, TokenTypes.VIDEO, TokenTypes.TEXT_FUSION, TokenTypes.TEXT,TokenTypes.IMAGE_FUSION,TokenTypes.IMAGE),
        max_pos_video: int = 5880,
        max_pos_image: int = 5880,
        max_pos_audio: int = 2048,
    ):

    #  
        super().__init__()
        self.max_return_tokens = len(return_token_types)

        self.return_token_types = return_token_types
        return_token_types_tensor = torch.tensor(list(map(lambda t: t.value, return_token_types)))
        self.register_buffer('return_token_types_tensor', return_token_types_tensor, persistent = False)

        self.return_tokens = nn.Parameter(torch.randn(self.max_return_tokens, dim))
        self.attn_pool = Attention(dim = dim, dim_head = dim_head, heads = heads)
        self.pos_embeds = nn.ModuleDict({
                    "audio": nn.Embedding(768, dim),
                    "video": nn.Embedding(768, dim),
                })
        self.dim = dim
        self.max_pos = {
            "audio": max_pos_audio,
            "video": max_pos_video,
            "image": max_pos_image
        }
        # audio input

        self.audio_patch_size = audio_patch_height, audio_patch_width = pair(audio_patch_size)

        self.spec = Spectrogram(
            n_fft = spec_n_fft,
            power = spec_power,
            win_length = spec_win_length,
            hop_length = spec_hop_length,
            pad = spec_pad,
            center = spec_center,
            pad_mode = spec_pad_mode
        )

        audio_input_dim = cum_mul(self.audio_patch_size)
        # print(audio_patch_size)
        self.audio_to_tokens = nn.Sequential(
            nn.Conv1d(
                in_channels=40,
                out_channels=audio_input_dim,
                kernel_size=audio_patch_size,
                stride=audio_patch_size
            ),
            Rearrange("b d t -> b t d"),
            nn.LayerNorm(audio_input_dim),
            nn.Linear(audio_input_dim, dim),
            nn.LayerNorm(dim)
        )

        # video input

        self.video_patch_size = (video_temporal_patch_size, *pair(video_patch_size))

        video_input_dim = cum_mul(self.video_patch_size) * video_channels
        video_patch_time, video_patch_height, video_patch_width = self.video_patch_size
        
        image_input_dim = cum_mul(self.video_patch_size) * video_channels
 # nn.Conv3d(
            #     in_channels=video_channels,
            #     out_channels=32,
            #     kernel_size=(1, 1, 1),
            #     stride=(1, 1, 1),
            #     padding=(0, 0, 0)
            # ),
            # nn.BatchNorm3d(256),
            # nn.ReLU(inplace=True),
            # nn.Conv3d(
            #     256, 512,
            #     kernel_size=(video_temporal_patch_size, video_patch_size, video_patch_size),
            #     stride=(video_temporal_patch_size, video_patch_size, video_patch_size),
            # ),
            # Rearrange("b d t w h -> b (t w h) d"),

        # video_grid = (18, 14, 14)           # pick an upper bound
        video_patch_time, video_patch_height, video_patch_width = self.video_patch_size
        self.video_grid = (
            max_pos_video // video_patch_time,          # temporal grid
            224 // video_patch_height,                  # height grid — or video.shape[-2] if dynamic
            224 // video_patch_width                    # width grid — or video.shape[-1] if dynamic
        )
        pos_3d = get_3d_sincos_pos_embed(
            embed_dim = dim,
            grid_size = self.video_grid,
            cls_token = False
          )   # returns [T*H*W, dim] numpy\
    

    
    
        self.register_buffer("video_pos_table",
            torch.from_numpy(pos_3d).float(), persistent=False)
        
        self.video_to_tokens = nn.Sequential(
            Rearrange('b c (t p1) (h p2) (w p3) -> b t h w (c p1 p2 p3)', p1 = video_patch_time, p2 = video_patch_height, p3 = video_patch_width),
            nn.LayerNorm(video_input_dim),
            nn.Linear(video_input_dim, dim),
            nn.LayerNorm(dim)
        )
        # self.image_to_tokens = nn.Sequential(
        #     nn.Conv2d(
        #         in_channels=3,
        #         out_channels=dim,
        #         kernel_size=(7, 7),
        #         stride=(7, 7)
        #     ),
        #     nn.BatchNorm2d(dim),
        #     nn.GELU(),
        #     Rearrange('b d h w -> b (h w) d'),
        #     nn.LayerNorm(dim),
        #     nn.Linear(dim, dim)
        # )



        # self.image_to_tokens = nn.Sequential(
        #     # Step 1: Downsample 224 → 112
        #     nn.Conv2d(3, dim // 2, kernel_size=3, stride=2, padding=1, bias=False),  # → [B, 48, 112, 112]
        #     nn.BatchNorm2d(dim // 2),
        #     nn.GELU(),

        #     # Step 2: Downsample 112 → 56
        #     nn.Conv2d(dim // 2, dim // 2, kernel_size=3, stride=2, padding=1, bias=False),  # → [B, 48, 56, 56]
        #     nn.BatchNorm2d(dim // 2),
        #     nn.GELU(),

        #     # Step 3: Downsample 56 → 24
        #     nn.Conv2d(dim // 2, dim // 2, kernel_size=5, stride=2, padding=2, bias=False),  # → [B, 48, 28, 28]
        #     nn.BatchNorm2d(dim // 2),
        #     nn.GELU(),

        #     nn.Conv2d(dim // 2, dim // 2, kernel_size=3, stride=1, padding=0, bias=False),  # → [B, 48, 26, 26]
        #     nn.BatchNorm2d(dim // 2),
        #     nn.GELU(),

        #     nn.Conv2d(dim // 2, dim // 2, kernel_size=3, stride=1, padding=1, bias=False),  # → [B, 48, 24, 24]
        #     nn.BatchNorm2d(dim // 2),
        #     nn.GELU(),

        #     # Step 4: Patchify 24x24 → 6x6
        #     nn.Conv2d(dim // 2, dim, kernel_size=4, stride=4, padding=0, bias=False),  # → [B, dim, 6, 6]
        #     nn.BatchNorm2d(dim),
        #     nn.GELU(),

        #     # Tokenize
        #     Rearrange('b c h w -> b h w c'),  # → [B, 6, 6, dim]
        #     nn.LayerNorm(dim),
        #     nn.Linear(dim, dim)  # optional projection
        # )

        # self.image_to_tokens = nn.Sequential( 
        #    *list(resnet50(pretrained=True).children())[:-3],
        #     nn.GELU(),
        #     Rearrange('b d h w -> b (h w) d'),
        #     nn.Linear(1024,512),
        #     nn.LayerNorm(dim)
        # )


        # self.image_to_tokens = nn.Sequential(
        #     # 1. low-level deep-fake feature extractor
        #     # nn.Conv2d(3, 32,  kernel_size=3, stride=2, padding=1, bias=False),
        #     # nn.GELU(),
        #     # nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False),  # ↓56
        #     # nn.GELU(),
        #     # nn.Conv2d(64, 256,kernel_size=4,stride=4,padding=0, bias=False),     

        #     # Rearrange('b d (h p1) (w p2) -> b (h w) (d p1 p2)', p1 = 4, p2 = 4),
        #     # nn.LayerNorm(4*256),
        #     # nn.Linear(4*256, dim),        


        #     Rearrange('b c (h ph) (w pw) -> b h w (c ph pw)', h=14, w=14),
        #     nn.LayerNorm(768),          # Because c * 16 * 16 = 3 * 256 = 768
        #     nn.Linear(768, dim),        # Project patch embedding to model dimension
        #     nn.LayerNorm(dim)

        #     # Rearrange('b c (h p2) (w p3) -> b h w (c p1 p2)', p1 = 28, p2 = 28),
        #     # Rearrange('b c (h ph) (w pw) -> b h w (c ph pw)', h=14, w=14),
        #     # nn.LayerNorm(588),
        #     # nn.Linear(588, dim),
        #     # nn.LayerNorm(dim)       
        
        # )
        
        self.image_grid = 224 // 3

        pos_2d = get_2d_sincos_pos_embed(
            embed_dim = dim,
            grid_size = self.image_grid,
            cls_token = False
          )   # returns [T*H*W, dim] numpy\

        self.register_buffer("image_pos_table",
            torch.from_numpy(pos_2d).float(), persistent=False)
        
        image_input_dim = video_patch_height *  video_patch_width * video_channels
        
        self.image_to_tokens = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b h w (c p1 p2)', p1 = video_patch_height, p2 = video_patch_width),
            nn.LayerNorm(image_input_dim),
            nn.Linear(image_input_dim, dim),
            nn.LayerNorm(dim)
        )
       

        

        # fusion tokens

        self.audio_fusion_tokens = nn.Parameter(torch.randn(num_fusion_tokens, dim))
        self.video_fusion_tokens = nn.Parameter(torch.randn(num_fusion_tokens, dim))
        self.image_fusion_tokens = nn.Parameter(torch.randn(num_fusion_tokens, dim))
        self.text_fusion_tokens = nn.Parameter(torch.randn(num_fusion_tokens, dim))

        # transformer

        self.layers = nn.ModuleList([])

        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                Attention(dim = dim, dim_head = dim_head, heads = heads),
                FeedForward(dim = dim, mult = ff_mult)
            ]))

        self.norm = LayerNorm(dim)

    # ---------------- private helpers ----------------
    def _ensure_pos_embed(self, modality: str, length: int):
        """Extend positional table on‑the‑fly if sequence is longer than current."""
        table: nn.Embedding = self.pos_embeds[modality]
        if length <= table.num_embeddings:
            return table
        new_len = min(self.max_pos[modality], max(length, table.num_embeddings * 2))
        if new_len <= table.num_embeddings:
            raise ValueError(f"{modality} sequence length {length} exceeds max_pos_{modality}={self.max_pos[modality]}")
        # print(new_len, self.dim)
        new_table = nn.Embedding(new_len, self.dim, device=table.weight.device)
        new_table.weight.data[: table.num_embeddings] = table.weight.data
        nn.init.normal_(new_table.weight.data[table.num_embeddings:])
        self.pos_embeds[modality] = new_table
        return new_table


    def forward(
        self,
        *,
        audio,
        video,
        text_tokens, 
        image,
        return_token_indices: Optional[Tuple[int]] = None
    ):
        # batch, device = audio.shape[0], audio.device
    
        # validate video can be patched
        if video is not None:
            assert all([divisible_by(numer, denom) for denom, numer in zip(self.video_patch_size, tuple(video.shape[-3:]))]), f'video shape {video.shape[-3:]} needs to be divisible by {self.video_patch_size}'

        # automatically crop if audio does not yield a 2d spectrogram that is divisible by patch sizes

        # audio = self.spec(audio)

        # height, width = audio.shape[-2:]
        # patch_height, patch_width = self.audio_patch_size

        # rounded_height, rounded_width = map(lambda args: round_down_nearest_multiple(*args), ((height, patch_height), (width, patch_width)))

        # if (height, width) != (rounded_height, rounded_width): # just keep printing to be annoying until it is fixed
        #     print_once(f'spectrogram yielded shape of {(height, width)}, but had to be cropped to {(rounded_height, rounded_width)} to be patchified for transformer')

        # audio = audio[..., :rounded_height, :rounded_width]

        # # to tokens
        # if audio is not None:
        #     audio_tokens = self.audio_to_tokens(audio)
        # else:
        #     audio_tokens = torch.randn()


        # if video is not None:
        #     video_tokens = self.video_to_tokens(video)
        # else:
        #     video_tokens = torch.randn()

            
        # if image is not None:
        #     image_tokens = self.image_to_tokens(image)
        # else:
        #     image_tokens = torch.randn()


        # if text_tokens is None:
        #     text_tokens = torch.randn()


        batch_size = audio.shape[0] if audio is not None else (
            video.shape[0] if video is not None else (
                image.shape[0] if image is not None else (
                    text_tokens.shape[0] if text_tokens is not None else 1
                )
            )
        ) 
        device = audio.device if audio is not None else (
            video.device if video is not None else (
                image.device if image is not None else (
                    text_tokens.device if text_tokens is not None else 1
                )
            )
        )
        if audio is not None:
            audio_tokens = self.audio_to_tokens(audio)
        else:
            audio_tokens = torch.zeros(batch_size, 1, self.dim, device=device)

        if video is not None:
            video_tokens = self.video_to_tokens(video)
        else:
            video_tokens = torch.zeros(batch_size, 1, self.dim, device=device)
        
        if image is not None:
            if image.shape[1] == 1 and image.shape[2] == 3:
                image = image.squeeze(1)
            
            image_tokens = self.image_to_tokens(image)
        else:
            image_tokens = torch.zeros(batch_size, 1, self.dim, device=device)

        if text_tokens is not None:
            text_tokens = text_tokens  # Already processed
        else:
            text_tokens = torch.zeros(batch_size, 1, self.dim, device=device)

        audio_fusion_tokens = repeat(self.audio_fusion_tokens, 'n d -> b n d', b = batch_size)
        video_fusion_tokens = repeat(self.video_fusion_tokens, 'n d -> b n d', b = batch_size)
        image_fusion_tokens = repeat(self.image_fusion_tokens, 'n d -> b n d', b = batch_size)
        text_fusion_tokens = repeat(self.text_fusion_tokens, 'n d -> b n d', b = batch_size)
        # print(audio_tokens.shape,video_tokens.shape)
        # construct all tokens
        audio_fusion_tokens, audio_tokens, video_fusion_tokens, video_tokens,text_fusion_tokens, text_tokens, image_fusion_tokens, image_tokens = map(lambda t: rearrange(t, 'b ... d -> b (...) d'), (audio_fusion_tokens,audio_tokens, video_fusion_tokens, video_tokens,text_fusion_tokens, text_tokens, image_fusion_tokens, image_tokens))
        

        # pos_table = self._ensure_pos_embed("audio", audio_tokens.size(1))
        # pos = pos_table.weight[: audio_tokens.size(1)][None]
        # audio_tokens = audio_tokens + pos
        
        v_pos = self.video_pos_table[:video_tokens.size(1)]
        # pos_table = self._ensure_pos_embed("video", video_tokens.size(1))
        # pos = pos_table.weight[: video_tokens.size(1)][None] 
        video_tokens = video_tokens + v_pos[None]

        i_pos = self.image_pos_table[:image_tokens.size(1)]
        # pos_table = self._ensure_pos_embed("video", video_tokens.size(1))
        # pos = pos_table.weight[: video_tokens.size(1)][None] 
        image_tokens = image_tokens + i_pos[None]

        tokens, ps = pack((
            audio_fusion_tokens,
            audio_tokens,
            video_fusion_tokens,
            video_tokens,
            text_fusion_tokens,
            text_tokens,
            image_fusion_tokens,
            image_tokens
        ), 'b * d')


        token_types = torch.tensor(list((
            *((TokenTypes.AUDIO_FUSION.value,) * audio_fusion_tokens.shape[-2]),
            *((TokenTypes.AUDIO.value,) * audio_tokens.shape[-2]),
            *((TokenTypes.VIDEO_FUSION.value,) * video_fusion_tokens.shape[-2]),
            *((TokenTypes.VIDEO.value,) * video_tokens.shape[-2]),
            *((TokenTypes.TEXT_FUSION.value,) * text_fusion_tokens.shape[-2]),
            *((TokenTypes.TEXT.value,) * text_tokens.shape[-2]),
            *((TokenTypes.IMAGE_FUSION.value,) * image_fusion_tokens.shape[-2]),
            *((TokenTypes.IMAGE.value,) * image_tokens.shape[-2]),
        )), device = device, dtype = torch.long)

        token_types_attend_from = rearrange(token_types, 'i -> i 1')
        token_types_attend_to = rearrange(token_types, 'j -> 1 j')

        # the logic goes
        # every modality, including fusion can attend to self

        m3mask = token_types_attend_from == token_types_attend_to

        # fusion can attend to everything

        m3mask = m3mask | (token_types_attend_from == TokenTypes.AUDIO_FUSION.value)
        m3mask = m3mask | (token_types_attend_from == TokenTypes.VIDEO_FUSION.value)
        m3mask = m3mask | (token_types_attend_from == TokenTypes.TEXT_FUSION.value)
        m3mask = m3mask | (token_types_attend_from == TokenTypes.IMAGE_FUSION.value)

        # attend and feedforward

        for attn, ff in self.layers:
            tokens = attn(tokens, attn_mask = m3mask) + tokens
            tokens = ff(tokens) + tokens

        tokens = self.norm(tokens)
        af_tok,a_tok, vf_tok, v_tok,tf_tok, t_tok, if_tok, i_tok= unpack(tokens, ps, "b * d")
        # final attention pooling - each modality pool token can only attend to its own tokens

        return_tokens = self.return_tokens
        return_token_types_tensor = self.return_token_types_tensor

        if exists(return_token_indices):
            assert len(set(return_token_indices)) == len(return_token_indices), 'all indices must be unique'
            assert all([indice < self.max_return_tokens for indice in return_token_indices]), 'indices must range from 0 to max_num_return_tokens - 1'

            return_token_indices = torch.tensor(return_token_indices, dtype = torch.long, device = device)

            return_token_types_tensor = return_token_types_tensor[return_token_indices]
            return_tokens = return_tokens[return_token_indices]

        return_tokens = repeat(return_tokens, 'n d -> b n d', b = batch_size)
        pool_mask = rearrange(return_token_types_tensor, 'i -> i 1') == token_types_attend_to
        # global queries can attend to all tokens
        pool_mask = pool_mask | rearrange(return_token_types_tensor, 'i -> i 1') == torch.ones_like(token_types_attend_to, dtype=torch.long) * TokenTypes.GLOBAL.value

        pooled_tokens = self.attn_pool(return_tokens, context = tokens, attn_mask = pool_mask) + return_tokens
        return pooled_tokens, af_tok,a_tok, vf_tok, v_tok,tf_tok, t_tok, if_tok, i_tok